"""
åŸºäºLangchainçš„Agenticé—®ç­”æµç¨‹ï¼šæ£€ç´¢Agentã€é—®ç­”Agentã€æ€»ç»“Agent
"""
import requests
import faiss
import numpy as np
import json
from sentence_transformers import SentenceTransformer
import re
from typing import List, Dict, Any, Optional
from langchain_core.documents import Document
from langchain_core.language_models import LLM
# from langchain.chains import LLMChain
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from pydantic import BaseModel, Field
import os

# è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
# è·å–é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.dirname(current_dir)

# 1. åŠ è½½æ¨¡å‹å’Œæ•°æ®
print("ğŸ” æ­£åœ¨åŠ è½½æ¨¡å‹å’Œå‘é‡åº“...")
model_path = os.path.join(project_root, "pre-process/hf_models", "text2vec-base-chinese")
model = SentenceTransformer(model_path)
index_path = os.path.join(current_dir, "faiss.index")
index = faiss.read_index(index_path)

chunks_path = os.path.join(current_dir, "chunks.json")
with open(chunks_path, "r", encoding="utf-8") as f:
    chunks = json.load(f)

laws_path = os.path.join(current_dir, "law_output.json")
with open(laws_path, "r", encoding="utf-8") as f:
    laws = json.load(f)

# æ„å»º id â†’ å®Œæ•´æ³•æ¡ æ˜ å°„
law_dict = {str(item["id"]): item for item in laws}

print(f"âœ… åŠ è½½å®Œæˆï¼Œç´¢å¼• {len(chunks)} æ®µï¼Œå®Œæ•´æ³•æ¡ {len(law_dict)} æ¡ã€‚\n")

# 2. è‡ªå®šä¹‰Ollama LLMç±»
class OllamaLLM(LLM):
    """è‡ªå®šä¹‰Ollama LLMç±»"""

    model_name: str = Field(default="llama3")
    temperature: float = Field(default=0.7)
    top_p: float = Field(default=0.9)
    num_predict: int = Field(default=512)
    repeat_penalty: float = Field(default=1.1)
    top_k: int = Field(default=40)

    @property
    def _llm_type(self) -> str:
        return "ollama"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """è°ƒç”¨Ollama API"""
        try:
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "temperature": self.temperature,
                    "top_p": self.top_p,
                    "num_predict": self.num_predict,
                    "repeat_penalty": self.repeat_penalty,
                    "top_k": self.top_k
                }
            )
            return response.json().get("response", "")
        except Exception as e:
            return f"ã€ç³»ç»Ÿé”™è¯¯ã€‘æ— æ³•è¿æ¥åˆ°æœ¬åœ°å¤§æ¨¡å‹ï¼š{e}\n\nè¯·æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œã€‚"

# 3. è‡ªå®šä¹‰æ£€ç´¢å™¨
class LawRetriever:
    """è‡ªå®šä¹‰æ³•å¾‹æ£€ç´¢å™¨"""

    def __init__(self, model, index, chunks, law_dict, top_k: int = 5):
        self.model = model
        self.index = index
        self.chunks = chunks
        self.law_dict = law_dict
        self.top_k = top_k

    def preprocess_query(self, query: str) -> str:
        """é¢„å¤„ç†æŸ¥è¯¢ï¼Œæå–å…³é”®ä¿¡æ¯"""
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
        query = re.sub(r'[^\w\s\u4e00-\u9fff]', '', query)
        return query.strip()

    def search_law(self, query: str) -> List[Dict]:
        """æœç´¢æ³•å¾‹æ¡æ–‡"""
        query_vec = self.model.encode([query])
        query_vec = query_vec / np.linalg.norm(query_vec, axis=1, keepdims=True)

        scores, indices = self.index.search(query_vec.astype("float32"), self.top_k * 2)

        results = []
        seen_ids = set()

        for score, idx in zip(scores[0], indices[0]):
            chunk_item = self.chunks[idx]
            law_id = str(chunk_item["id"])

            if law_id in seen_ids:
                continue

            full_law = self.law_dict.get(law_id, {})

            if full_law.get("content"):
                results.append({
                    "id": law_id,
                    "category": full_law.get("category", "æœªçŸ¥"),
                    "title": full_law.get("title", "æœªçŸ¥"),
                    "content": full_law.get("content", "ï¼ˆæœªæ‰¾åˆ°å®Œæ•´æ³•æ¡ï¼‰"),
                    "score": float(score)
                })
                seen_ids.add(law_id)

                if len(results) >= self.top_k:
                    break

        return results

    def _get_relevant_documents(self, query: str) -> List[Document]:
        """è·å–ç›¸å…³æ–‡æ¡£"""
        processed_query = self.preprocess_query(query)
        results = self.search_law(processed_query)

        documents = []
        for result in results:
            # æ„å»ºæ–‡æ¡£å†…å®¹
            content = f"æ³•æ¡ID: {result['id']}\nåˆ†ç±»: {result['category']}\næ ‡é¢˜: {result['title']}\nå†…å®¹: {result['content']}"

            doc = Document(
                page_content=content,
                metadata={
                    "id": result["id"],
                    "category": result["category"],
                    "title": result["title"],
                    "score": result["score"]
                }
            )
            documents.append(doc)

        return documents

# 4. åˆ›å»ºLangchainç»„ä»¶
# åˆå§‹åŒ–LLM
llm = OllamaLLM()

# åˆå§‹åŒ–æ£€ç´¢å™¨
retriever = LawRetriever(model, index, chunks, law_dict, top_k=8)

# 5. é—®ç­”Agentçš„Promptæ¨¡æ¿
qa_prompt_template = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ä¸­å›½æ³•å¾‹ä¸“å®¶ï¼Œå…·æœ‰ä¸°å¯Œçš„æ³•å¾‹å®åŠ¡ç»éªŒã€‚è¯·æ ¹æ®æä¾›çš„ç›¸å…³æ³•æ¡ï¼Œä¸ºç”¨æˆ·æä¾›ä¸“ä¸šã€å‡†ç¡®çš„æ³•å¾‹å’¨è¯¢ã€‚

ã€ç›¸å…³æ³•æ¡ã€‘
{context}

ã€å†å²å¯¹è¯ã€‘
{history}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€å›ç­”è¦æ±‚ã€‘
1. è¯­è¨€ï¼šå¿…é¡»ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œé¿å…ä½¿ç”¨è‹±æ–‡
2. ç»“æ„ï¼šæŒ‰ç…§"æ³•å¾‹ä¾æ®â†’å…·ä½“åˆ†æâ†’å®åŠ¡å»ºè®®"çš„ç»“æ„å›ç­”
3. å¼•ç”¨ï¼šæ˜ç¡®å¼•ç”¨ç›¸å…³æ³•æ¡çš„å…·ä½“æ¡æ¬¾
4. å®åŠ¡ï¼šæä¾›å…·ä½“çš„æ“ä½œå»ºè®®å’Œæ³¨æ„äº‹é¡¹
5. å‡†ç¡®ï¼šç¡®ä¿æ³•å¾‹ä¾æ®å‡†ç¡®ï¼Œé¿å…è¯¯å¯¼

ã€å›ç­”æ ¼å¼ã€‘
æ ¹æ®ã€ŠXXXæ³•ã€‹ç¬¬Xæ¡è§„å®šï¼Œ...

å…·ä½“åˆ†æï¼š
...

å®åŠ¡å»ºè®®ï¼š
1. ...
2. ...

å›ç­”ï¼š"""

qa_prompt = PromptTemplate(
    input_variables=["context", "history", "question"],
    template=qa_prompt_template
)

# 6. æ€»ç»“Agentçš„Promptæ¨¡æ¿
summary_prompt_template = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ³•å¾‹é¡¾é—®ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„å…·ä½“é—®é¢˜ï¼Œå¯¹ä»¥ä¸‹æ³•å¾‹é—®ç­”å†…å®¹è¿›è¡Œä¸“ä¸šã€å‡†ç¡®çš„ä¸­æ–‡æ€»ç»“ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€é—®ç­”å†…å®¹ã€‘
{content}

ã€æ€»ç»“è¦æ±‚ã€‘
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æ„è¿›è¡Œæ€»ç»“ï¼Œå¿…é¡»åŒ…å«è¿™å››ä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªè¦ç‚¹éƒ½è¦å•ç‹¬ä¸€è¡Œï¼š

ğŸ“‹ é—®é¢˜æ€»ç»“
â€¢ é’ˆå¯¹ç”¨æˆ·é—®é¢˜çš„ç¡®åˆ‡å›ç­”ï¼š[æ ¹æ®ç”¨æˆ·çš„å…·ä½“é—®é¢˜ï¼Œç”¨ä¸€å¥è¯å‡†ç¡®å›ç­”ç”¨æˆ·æœ€å…³å¿ƒçš„æ ¸å¿ƒé—®é¢˜]

ğŸ“‹ æ ¸å¿ƒè¦ç‚¹
â€¢ ä¸»è¦æ³•å¾‹é—®é¢˜ï¼š[å…·ä½“å†…å®¹]
â€¢ å…³é”®æ³•å¾‹ä¾æ®ï¼š[å…·ä½“å†…å®¹]
â€¢ æ ¸å¿ƒäº‰è®®ç‚¹ï¼š[å…·ä½“å†…å®¹]

âš–ï¸ æ³•å¾‹æ¡æ¬¾
â€¢ ç›¸å…³æ³•æ¡åç§°ï¼š[å…·ä½“å†…å®¹]
â€¢ å…·ä½“æ¡æ¬¾å†…å®¹ï¼š[å…·ä½“å†…å®¹]
â€¢ é€‚ç”¨æ¡ä»¶ï¼š[å…·ä½“å†…å®¹]

ğŸ’¡ å®åŠ¡å»ºè®®
â€¢ æ“ä½œæ­¥éª¤ï¼š[å…·ä½“å†…å®¹]
â€¢ æ³¨æ„äº‹é¡¹ï¼š[å…·ä½“å†…å®¹]
â€¢ é£é™©æç¤ºï¼š[å…·ä½“å†…å®¹]
â€¢ ç»´æƒé€”å¾„ï¼š[å…·ä½“å†…å®¹]

è¦æ±‚ï¼š
- å¿…é¡»ä½¿ç”¨ä¸­æ–‡
- ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°å››ä¸ªéƒ¨åˆ†ç»“æ„
- æ¯ä¸ªè¦ç‚¹éƒ½è¦å•ç‹¬ä¸€è¡Œï¼Œä½¿ç”¨"â€¢"ç¬¦å·
- æ¯ä¸ªéƒ¨åˆ†éƒ½è¦æœ‰å…·ä½“å†…å®¹
- è¯­è¨€ä¸“ä¸šå‡†ç¡®
- ç»“æ„æ¸…æ™°æ˜äº†
- é‡ç‚¹çªå‡º
- é¿å…ä½¿ç”¨è‹±æ–‡
- æ¯ä¸ªè¦ç‚¹ä¹‹é—´è¦æœ‰æ¢è¡Œ
- å¼€å¤´å¿…é¡»æ ¹æ®ç”¨æˆ·çš„å…·ä½“é—®é¢˜ç»™å‡ºå‡†ç¡®çš„å›ç­”

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
ğŸ“‹ é—®é¢˜æ€»ç»“
â€¢ [æ ¹æ®ç”¨æˆ·çš„å…·ä½“é—®é¢˜ï¼Œç”¨ä¸€å¥è¯å‡†ç¡®å›ç­”ç”¨æˆ·æœ€å…³å¿ƒçš„æ ¸å¿ƒé—®é¢˜]

ğŸ“‹ æ ¸å¿ƒè¦ç‚¹
â€¢ [å…·ä½“å†…å®¹]
â€¢ [å…·ä½“å†…å®¹]
â€¢ [å…·ä½“å†…å®¹]

âš–ï¸ æ³•å¾‹æ¡æ¬¾
â€¢ [å…·ä½“å†…å®¹]
â€¢ [å…·ä½“å†…å®¹]
â€¢ [å…·ä½“å†…å®¹]

ğŸ’¡ å®åŠ¡å»ºè®®
â€¢ [å…·ä½“å†…å®¹]
â€¢ [å…·ä½“å†…å®¹]
â€¢ [å…·ä½“å†…å®¹]

æ€»ç»“ï¼š"""

summary_prompt = PromptTemplate(
    input_variables=["question", "content"],
    template=summary_prompt_template
)

# 7. æ„å»ºLangchainé“¾
# é—®ç­”é“¾
qa_chain = qa_prompt | llm | StrOutputParser()

# æ€»ç»“é“¾
summary_chain = summary_prompt | llm | StrOutputParser()


# 8. Agentå‡½æ•°
def search_agent(query: str, topk: int = 10) -> List[tuple]:
    """æ£€ç´¢Agent - ä½¿ç”¨Langchainæ£€ç´¢å™¨"""
    # ç›´æ¥è°ƒç”¨æ£€ç´¢å™¨çš„æœç´¢æ–¹æ³•
    processed_query = retriever.preprocess_query(query)
    results = retriever.search_law(processed_query)

    retrieved_texts = []
    for result in results:
        # æ„å»ºæ–‡æ¡£å†…å®¹
        content = f"æ³•æ¡ID: {result['id']}\nåˆ†ç±»: {result['category']}\næ ‡é¢˜: {result['title']}\nå†…å®¹: {result['content']}"
        retrieved_texts.append((
            content,
            result["score"],
            result["id"]
        ))

    return retrieved_texts

def qa_agent(query: str, retrieved_texts: List[tuple], history: Optional[List[Dict]] = None) -> str:
    """é—®ç­”Agent - ä½¿ç”¨Langchainé“¾"""
    if not retrieved_texts:
        return "æŠ±æ­‰ï¼Œæœªèƒ½æ‰¾åˆ°ç›¸å…³çš„æ³•å¾‹æ¡æ–‡ã€‚è¯·å°è¯•ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®ã€‚"

    # æ„å»ºä¸Šä¸‹æ–‡
    context_parts = []
    for i, (text, score, chunk_id) in enumerate(retrieved_texts, 1):
        context_parts.append(f"[æ³•æ¡{i}] ç›¸å…³åº¦:{score:.3f}\n{text}")

    context = '\n\n'.join(context_parts)

    # æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡
    history_text = ""
    if history:
        history_contexts = []
        for h in history[-2:]:  # åªä¿ç•™æœ€è¿‘2è½®å¯¹è¯
            history_contexts.append(f"ç”¨æˆ·ï¼š{h['user']}\nåŠ©æ‰‹ï¼š{h['assistant']}")
        history_text = "\n\nå†å²å¯¹è¯ï¼š\n" + "\n".join(history_contexts)

    # è°ƒç”¨Langchainé“¾
    result = qa_chain.invoke({
        "context": context,
        "history": history_text,
        "question": query
    })

    return result

def summary_agent(answers: List[str], user_question: Optional[str] = None) -> str:
    """æ€»ç»“Agent - ä½¿ç”¨Langchainé“¾"""
    if not answers:
        return "æš‚æ— å†…å®¹å¯æ€»ç»“"

    # å°†æ‰€æœ‰å›ç­”åˆå¹¶
    content = "\n".join(answers)

    # è°ƒç”¨Langchainé“¾
    result = summary_chain.invoke({
        "question": user_question or "æœªçŸ¥é—®é¢˜",
        "content": content
    })

    return f"ã€æ™ºèƒ½æ€»ç»“ã€‘\n{result}"

# ä¸»ç¨‹åºï¼šç”¨æˆ·äº¤äº’é€»è¾‘
if __name__ == '__main__':
    history: List[Dict[str, str]] = []
    print("æ¬¢è¿ä½¿ç”¨åŸºäºLangchainçš„å¤šè½®æ³•å¾‹é—®ç­”ç³»ç»Ÿï¼Œè¾“å…¥ exit é€€å‡ºã€‚")

    while True:
        query = input('è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆæŒ‰å›è½¦æ£€ç´¢ï¼Œè¾“å…¥ exit é€€å‡ºï¼‰ï¼š')
        if query.strip().lower() == 'exit':
            break

        retrieved = search_agent(query, topk=8)
        print('\nã€æ£€ç´¢Agentç»“æœã€‘')
        for idx, (text, score, chunk_id) in enumerate(retrieved, 1):
            # ä»law_dictè·å–å®Œæ•´ä¿¡æ¯
            full_law = law_dict.get(str(chunk_id), {})
            law_name = full_law.get('category', 'æœªçŸ¥æ³•å¾‹')
            law_id = chunk_id
            law_content = full_law.get('content', 'ï¼ˆæœªæ‰¾åˆ°å®Œæ•´æ³•æ¡ï¼‰')

            print(f'{idx}. ã€Š{law_name}ã€‹')
            print(f'   ID: {law_id}')
            print(f'   ç›¸ä¼¼åº¦: {score:.4f}')
            print(f'   å®Œæ•´æ¡æ–‡: {law_content}')
            print('-' * 50)

        answer = qa_agent(query, retrieved, history)
        print('\nã€é—®ç­”Agentç»“æœã€‘')
        print(answer)

        summary = summary_agent([answer], user_question=query)
        print('\nã€æ€»ç»“Agentç»“æœã€‘')
        print(summary)

        print('-' * 40)
        # è®°å½•å†å²é—®ç­”
        history.append({"user": query, "assistant": answer})
