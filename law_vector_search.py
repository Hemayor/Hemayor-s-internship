import json
import faiss
import os
import re
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List, Dict

# ---------- å‚æ•°è®¾ç½® ----------
INPUT_JSON = "law_output.json"
OUTPUT_CHUNKS_JSON = "chunks.json"
OUTPUT_INDEX_FILE = "faiss.index"
CHUNK_SIZE = 300  # æ¯æ®µæœ€å¤šå¤šå°‘å­—ç¬¦
CHUNK_OVERLAP = 30  # é‡å å­—ç¬¦æ•°

# ---------- åˆå§‹åŒ–æ¨¡å‹ ----------
print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹ SentenceTransformer...")
model = SentenceTransformer('hf_models/text2vec-base-chinese')
embedding_dim = model.get_sentence_embedding_dimension()
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè¾“å‡ºç»´åº¦ä¸ºï¼š{embedding_dim} ç»´\n")


# ---------- åˆ†æ®µå‡½æ•° ----------
def split_text_with_overlap(text: str, max_len: int, overlap: int) -> List[str]:
    # 1. æŒ‰å¥å·åˆ†å‰²
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])', text)  # åŒ…æ‹¬ï¼šã€‚ã€ï¼ï¼Ÿ
    chunks = []
    current_chunk = ""

    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue

        # å¦‚æœè¿˜æœªè¶…é•¿ï¼ŒåŠ åˆ°åˆ†æ®µé‡Œ
        if len(current_chunk) + len(sentence) <= max_len:
            current_chunk += sentence
        else:
            if current_chunk:
                chunks.append(current_chunk)

            if overlap > 0 and chunks:
                overlap_text = chunks[-1][-overlap:]
                current_chunk = overlap_text + sentence
            else:
                current_chunk = sentence

    if current_chunk:
        chunks.append(current_chunk)

    return chunks


# ---------- è¯»å–æ³•æ¡æ•°æ® ----------
print(f"ğŸ“… æ­£åœ¨è¯»å–æ³•æ¡æ•°æ®ï¼š{INPUT_JSON}")
with open(INPUT_JSON, "r", encoding="utf-8") as f:
    data = json.load(f)
print(f"âœ… è¯»å–æˆåŠŸï¼Œå…± {len(data)} æ¡æ³•æ¡\n")


# ---------- æ„é€ åˆ†æ®µä¸å…ƒæ•°æ® ----------
print("ğŸ”§ æ­£åœ¨è¿›è¡Œæ™ºèƒ½é‡å åˆ†æ®µå¤„ç†...")
chunks = []
metadatas = []

for entry in data:
    full_text = entry["content"].replace("\n", "").strip()
    chunk_list = split_text_with_overlap(full_text, CHUNK_SIZE, CHUNK_OVERLAP)

    for chunk in chunk_list:
        full_chunk_text = f"{entry['category']} {entry['title']} {chunk}"
        chunks.append(full_chunk_text)
        metadatas.append({
            "id": entry["id"],
            "category": entry["category"],
            "article": entry["title"],
            "chunk": chunk
        })

print(f"âœ… åˆ†æ®µå®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} æ®µæ–‡æœ¬\n")


# ---------- æ–‡æœ¬å‘é‡åŒ– ----------
print("ğŸ§  æ­£åœ¨è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–ï¼ˆembeddingï¼‰...")
embeddings = model.encode(chunks, show_progress_bar=True, normalize_embeddings=True)
print(f"âœ… å‘é‡åŒ–å®Œæˆï¼Œå‘é‡å½¢çŠ¶ä¸ºï¼š{embeddings.shape}\n")


# ---------- æ„å»º FAISS ç´¢å¼• ----------
print("ğŸ“¦ æ­£åœ¨æ„å»º FAISS ç´¢å¼•...")
index = faiss.IndexFlatIP(embedding_dim)  # IP = inner product ä»£è¡¨ cosine ç›¸ä¼¼åº¦
index.add(embeddings)
faiss.write_index(index, OUTPUT_INDEX_FILE)
print(f"âœ… ç´¢å¼•å·²ä¿å­˜è‡³ï¼š{OUTPUT_INDEX_FILE}\n")


# ---------- ä¿å­˜ chunks å…ƒæ•°æ® ----------
print("ğŸ’¾ æ­£åœ¨ä¿å­˜æ®µè½å…ƒæ•°æ®è‡³ JSON æ–‡ä»¶...")
with open(OUTPUT_CHUNKS_JSON, "w", encoding="utf-8") as f:
    json.dump(metadatas, f, ensure_ascii=False, indent=2)
print(f"âœ… æ®µè½ä¿¡æ¯å·²ä¿å­˜è‡³ï¼š{OUTPUT_CHUNKS_JSON}\n")

print("ğŸ‰ æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæ¯•ï¼æ³•æ¡æ•°æ®å·²ç»å‡†å¤‡å¥½ç”¨äºå‘é‡æ£€ç´¢ã€‚")
