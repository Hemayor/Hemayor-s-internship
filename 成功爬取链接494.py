from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import os

# -------------------- é…ç½® --------------------
BASE_URL = "https://www.pkulaw.com"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
    'Referer': BASE_URL
}

# -------------------- æå–é“¾æ¥å‡½æ•° --------------------
def get_law_detail_urls(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    detail_urls = []
    for a in soup.select('h4 a[href*="/chl/"]'):
        href = a['href']
        clean_url = href.split('?')[0]
        full_url = urljoin(BASE_URL, clean_url)
        detail_urls.append(full_url)
    return detail_urls

# -------------------- ä¸»æµç¨‹ --------------------
if __name__ == "__main__":
    options = Options()
    # options.add_argument("--headless")  # å¦‚éœ€éšè—æµè§ˆå™¨çª—å£å–æ¶ˆæ³¨é‡Š
    driver = webdriver.Chrome(options=options)

    driver.get(BASE_URL)
    print("ğŸ‘‰ è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ã€ç­›é€‰ã€æ»‘å—éªŒè¯ç­‰æ“ä½œã€‚")
    input("âœ… å®ŒæˆåæŒ‰å›è½¦ç»§ç»­...")

    wait = WebDriverWait(driver, 30)
    visited_urls = set()
    all_urls = []
    prev_first_url = ""

    for page in range(1, 6):
        print(f"\nğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {page} é¡µ...")

        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.accompanying-wrap h4 a")))
        except:
            input("âš ï¸ é¡µé¢åŠ è½½å¤±è´¥ï¼Œå¯èƒ½éœ€è¦æ»‘å—éªŒè¯ï¼Œè¯·æ‰‹åŠ¨å¤„ç†åæŒ‰å›è½¦...")

        page_source = driver.page_source
        detail_urls = get_law_detail_urls(page_source)

        # åˆ¤æ–­é¡µé¢æ˜¯å¦é‡å¤ï¼ˆé¦–ä¸ªé“¾æ¥ç›¸åŒï¼‰
        if detail_urls and detail_urls[0] == prev_first_url:
            print("âš ï¸ å½“å‰é¡µä¸ä¸Šä¸€é¡µå†…å®¹é‡å¤ï¼Œå¯èƒ½é¡µé¢æœªåˆ·æ–°æˆ–é‡åˆ°æ»‘å—éªŒè¯")
            input("ğŸ‘‰ è¯·åœ¨æµè§ˆå™¨å®ŒæˆéªŒè¯æˆ–æ‰‹åŠ¨ç‚¹å‡»â€œä¸‹ä¸€é¡µâ€ï¼Œå®ŒæˆåæŒ‰å›è½¦...")

            # å†æ¬¡å°è¯•è·å–é¡µé¢
            page_source = driver.page_source
            detail_urls = get_law_detail_urls(page_source)
            if detail_urls and detail_urls[0] == prev_first_url:
                print("âŒ é¡µé¢ä»ç„¶é‡å¤ï¼Œè·³è¿‡æ­¤é¡µã€‚")
                continue

        # ä¿å­˜å½“å‰é¡µçš„é¦–é“¾æ¥
        if detail_urls:
            prev_first_url = detail_urls[0]

        print(f"ğŸ”— è·å–åˆ° {len(detail_urls)} ä¸ªé“¾æ¥")
        for url in detail_urls:
            if url not in visited_urls:
                visited_urls.add(url)
                all_urls.append(url)

        # ç‚¹å‡»â€œä¸‹ä¸€é¡µâ€
        if page < 5:
            try:
                next_btn = wait.until(EC.element_to_be_clickable((By.LINK_TEXT, "ä¸‹ä¸€é¡µ")))
                next_btn.click()
                time.sleep(4)  # ç­‰å¾…åŠ è½½
            except Exception as e:
                print(f"âš ï¸ æ— æ³•ç‚¹å‡»ä¸‹ä¸€é¡µï¼š{e}")
                input("ğŸ‘‰ è¯·æ‰‹åŠ¨ç‚¹å‡»â€œä¸‹ä¸€é¡µâ€å¹¶å¤„ç†éªŒè¯ç ï¼Œç„¶åæŒ‰å›è½¦ç»§ç»­...")

    # ä¿å­˜é“¾æ¥åˆ°æ–‡ä»¶
    with open("è¯¦æƒ…é¡µé“¾æ¥.txt", "w", encoding="utf-8") as f:
        for i, url in enumerate(all_urls, 1):
            f.write(f"{i}. {url}\n")

    print(f"\nâœ… å…±è·å– {len(all_urls)} æ¡é“¾æ¥ï¼Œå·²ä¿å­˜åˆ° è¯¦æƒ…é¡µé“¾æ¥.txt")
    driver.quit()
